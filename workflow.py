"""
Enhanced Estimation Workflow using LangGraph Orchestrator-Worker Pattern
=======================================================================

Ki·∫øn tr√∫c m·ªõi v·ªõi 3 workers chuy√™n bi·ªát:
1. Worker 1: Task Breakdown v·ªõi GraphRAG integration
2. Worker 2: Estimation Worker cho effort calculation
3. Worker 3: Effort Calculator & Validator v·ªõi validation logic

T√≠ch h·ª£p v·ªõi GraphRAG handler t·ª´ app.py ƒë·ªÉ ph√¢n t√≠ch th√¥ng minh h∆°n.

Author: AI Assistant
Date: 2025-09-28
"""

import os
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, TypedDict, Annotated, Tuple, Optional
import operator
from dataclasses import dataclass, field
import uuid

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langgraph.checkpoint.memory import MemorySaver
from streamlit import form

# Import logging
from utils.logger import get_logger
from config import Config

# Initialize logger
logger = get_logger(__name__)

# ========================
# LLM Response Dump Helper
# ========================

def dump_llm_response(worker_name: str, task_info: str, raw_response: str, parsed_result: Any = None, project_id: str = None):
    """
    Dump LLM response to file for debugging.
    
    Args:
        worker_name: Name of the worker (orchestrator, breakdown_worker, estimation_worker, validation_worker)
        task_info: Info about the task being processed
        raw_response: Raw LLM response content
        parsed_result: Parsed JSON result (if available)
        project_id: Project identifier for project-scoped storage
    """
    try:
        # Determine logs directory (project-scoped or default)
        if project_id:
            logs_dir = os.path.join(Config.LOG_DIR, project_id, 'llm_responses')
        else:
            logs_dir = os.path.join(Config.LOG_DIR, 'llm_responses')
        
        os.makedirs(logs_dir, exist_ok=True)
        
        # Create timestamped filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        filename = f"{worker_name}_{timestamp}.json"
        filepath = os.path.join(logs_dir, filename)
        
        # Prepare dump data
        dump_data = {
            'timestamp': datetime.now().isoformat(),
            'worker_name': worker_name,
            'task_info': task_info,
            'raw_response': raw_response,
            'raw_response_length': len(raw_response),
            'parsed_result': parsed_result,
            'parsing_success': parsed_result is not None
        }
        
        # Write to file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(dump_data, f, indent=2, ensure_ascii=False)
        
        logger.debug(f"   üíæ Dumped LLM response to: {filepath}")
        
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Failed to dump LLM response: {e}")

# ========================
# Enhanced Data Models
# ========================

@dataclass
class TaskBreakdown:
    """
    Enhanced model cho vi·ªác break task v·ªõi validation.
    
    ARCHITECTURE:
    - Each task represents ONE functional/business requirement
    - Task name describes business logic (e.g., "User Login Feature", "Product Search")
    - Effort is broken down by ROLE √ó TASK TYPE in separate columns:
      * backend_implement, backend_fixbug, backend_unittest
      * frontend_implement, frontend_fixbug, frontend_unittest
      * responsive_implement
      * testing_implement (QA manual testing)
    - Export shows ONE row per task with all role efforts in columns
    - Internal breakdown (BE/FE/QA analysis) is used for accuracy but aggregated on export
    """
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    category: str = ""  # Business logic category (Authentication, User Management, etc.)
    parent_task: str = ""  # Parent functional requirement (if hierarchical)
    task_name: str = ""  # Business logic task name (e.g., "User Login Feature")
    description: str = ""  # Detailed description covering all aspects (BE + FE + QA)
    estimation_manday: float = 0.0  # Total estimation (sum of ALL role-specific efforts)
    complexity: str = "Medium"  # Low, Medium, High
    priority: str = "Medium"  # Low, Medium, High
    confidence_level: float = 0.8  # 0.0 - 1.0

    # Sun Asterisk-specific fields
    sub_no: str = ""  # Sub.No format: X.Y (X=category, Y=task number)
    premise: str = ""  # Premise
    remark: str = ""  # ÂÇôËÄÉ Remark
    note: str = ""  # Note
    
    # Detailed effort breakdown by ROLE √ó TASK TYPE
    # Backend efforts
    backend_implement: float = 0.0
    backend_fixbug: float = 0.0
    backend_unittest: float = 0.0
    
    # Frontend efforts
    frontend_implement: float = 0.0
    frontend_fixbug: float = 0.0
    frontend_unittest: float = 0.0
    
    # Responsive design
    responsive_implement: float = 0.0
    
    # QA/Testing efforts (manual testing)
    testing_implement: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        return {
            'id': self.id,
            'category': self.category,
            'parent_task': self.parent_task,
            'task_name': self.task_name,
            'description': self.description,
            'estimation_manday': self.estimation_manday,
            'complexity': self.complexity,
            'priority': self.priority,
            'confidence_level': self.confidence_level,
            # Sun Asterisk fields
            'sub_no': self.sub_no,
            'premise': self.premise,
            'remark': self.remark,
            'note': self.note,
            # Role-specific effort breakdown
            'backend_implement': self.backend_implement,
            'backend_fixbug': self.backend_fixbug,
            'backend_unittest': self.backend_unittest,
            'frontend_implement': self.frontend_implement,
            'frontend_fixbug': self.frontend_fixbug,
            'frontend_unittest': self.frontend_unittest,
            'responsive_implement': self.responsive_implement,
            'testing_implement': self.testing_implement
        }

    def to_sunasterisk_format(self) -> Dict[str, Any]:
        """Convert to Sun Asterisk Excel format - each task is one row with all role efforts."""
        return {
            'category': self.category,
            'parent_task': self.parent_task,
            'sub_task': self.task_name,  # Business logic task name
            'sub_no': self.sub_no,
            'premise': self.premise,
            'remark': self.remark,
            'backend': {
                'implement': self.backend_implement,
                'fixbug': self.backend_fixbug,
                'unittest': self.backend_unittest
            },
            'frontend': {
                'implement': self.frontend_implement,
                'fixbug': self.frontend_fixbug,
                'unittest': self.frontend_unittest
            },
            'responsive': {
                'implement': self.responsive_implement
            },
            'testing': {
                'implement': self.testing_implement
            },
            'note': self.note
        }

@dataclass
class GraphRAGInsight:
    """Model ƒë·ªÉ l∆∞u insights t·ª´ GraphRAG"""
    query: str
    response: str
    references: List[str]
    timestamp: str
    confidence: float = 0.8

# ========================
# Enhanced State Definitions
# ========================

class EnhancedOrchestratorState(TypedDict):
    """
    Enhanced state cho Orchestrator v·ªõi GraphRAG integration
    UPDATED: Keeps validated_results for backward compatibility with Option 3 (conditional validation)
    """
    project_id: str  # Project identifier for project-scoped operations
    original_task: str  # Task g·ªëc t·ª´ user
    graphrag_insights: List[Dict[str, Any]]  # Insights t·ª´ GraphRAG queries

    # Category planning
    main_categories: List[str]  # C√°c category ch√≠nh

    # Worker results v·ªõi consistent annotations
    breakdown_results: Annotated[List[Dict[str, Any]], operator.add]  # K·∫øt qu·∫£ t·ª´ Worker 1
    estimation_results: Annotated[List[Dict[str, Any]], operator.add]  # K·∫øt qu·∫£ t·ª´ Worker 2 (includes Option 1 buffer)
    validated_results: Annotated[List[Dict[str, Any]], operator.add]  # K·∫øt qu·∫£ t·ª´ Worker 3 (Option 3: conditional only)

    # Final outputs
    final_estimation_data: List[Dict[str, Any]]  # Final serializable data (merged from estimation + validated)
    total_effort: float  # T·ªïng effort (manday)
    total_confidence: float  # Confidence score trung b√¨nh
    mermaid_diagram: str  # Mermaid code cho visualization
    validation_summary: Dict[str, Any]  # Summary c·ªßa validation process
    workflow_status: str  # Tr·∫°ng th√°i workflow

# ========================
# Enhanced LLM Configuration
# ========================

class EnhancedEstimationLLM:
    """Enhanced LLM wrapper v·ªõi prompts chuy√™n bi·ªát cho t·ª´ng worker"""

    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.1,
            max_tokens=3000
        )

    def get_orchestrator_prompt(self) -> str:
        return """
        B·∫°n l√† m·ªôt Senior Project Manager v·ªõi 15 nƒÉm kinh nghi·ªám trong vi·ªác ph√¢n t√≠ch v√† breakdown c√°c d·ª± √°n ph·∫ßn m·ªÅm ph·ª©c t·∫°p.

        Nhi·ªám v·ª• c·ªßa b·∫°n:
        1. Ph√¢n t√≠ch task ƒë∆∞·ª£c cung c·∫•p v·ªõi context t·ª´ GraphRAG
        2. X√°c ƒë·ªãnh c√°c BUSINESS LOGIC CATEGORIES ch√≠nh c·∫ßn cho d·ª± √°n
        3. T·∫°o chi·∫øn l∆∞·ª£c ƒë·ªÉ breakdown task m·ªôt c√°ch to√†n di·ªán
        4. Chu·∫©n b·ªã input cho c√°c workers chuy√™n bi·ªát

        QUAN TR·ªåNG: Categories ph·∫£i l√† business logic categories (kh√¥ng ph·∫£i technical roles):
        - Authentication & Authorization
        - User Management
        - Product Management
        - Order Management
        - Payment Processing
        - Reporting & Analytics
        - Notification System
        - Content Management
        - Search & Filtering
        - Admin Dashboard
        - API Integration
        - Security & Compliance
        - Documentation
        v.v...

        B·∫°n s·∫Ω c√≥ th√¥ng tin t·ª´ GraphRAG ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ context v√† requirements.

        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi format:
        {
            "categories": ["Authentication & Authorization", "User Management", "Product Management", "Reporting & Analytics", "Notification System", "Documentation"],
            "analysis_strategy": "Chi·∫øn l∆∞·ª£c ph√¢n t√≠ch t·ªïng th·ªÉ",
            "complexity_assessment": "Low/Medium/High",
            "estimated_timeline": "∆Ø·ªõc t√≠nh th·ªùi gian t·ªïng th·ªÉ"
        }
        """

    def get_breakdown_worker_prompt(self) -> str:
        return """
        B·∫°n l√† m·ªôt Technical Lead chuy√™n gia trong vi·ªác break down technical requirements th√†nh c√°c task c·ª• th·ªÉ.

        üéØ KI·∫æN TR√öC M·ªöI - BUSINESS LOGIC TASK BREAKDOWN:
        ===================================================
        M·ªñI TASK ƒê·∫†I DI·ªÜN CHO M·ªòT FUNCTIONAL/BUSINESS REQUIREMENT:
        
        ‚úÖ Task n√™n ƒë∆∞·ª£c chia theo BUSINESS LOGIC, KH√îNG ph·∫£i theo technical roles (BE/FE/QA)
        ‚úÖ Task name m√¥ t·∫£ ch·ª©c nƒÉng nghi·ªáp v·ª•: "User Login", "Product Search", "Order Checkout"
        ‚úÖ Description bao g·ªìm T·∫§T C·∫¢ c√°c kh√≠a c·∫°nh:
           - Backend requirements (API, business logic, database)
           - Frontend requirements (UI, user interactions, components)
           - Testing considerations (test cases, edge cases)
        
        üìä EFFORT ESTIMATION S·∫º ƒê∆Ø·ª¢C X·ª¨ L√ù SAU:
        =========================================
        - Estimation Worker s·∫Ω ph√¢n t√≠ch task v√† estimate effort cho T·ª™NG ROLE
        - K·∫øt qu·∫£ export s·∫Ω c√≥ c√°c COLUMNS ri√™ng cho m·ªói role:
          * Backend Implement, Backend FixBug, Backend Unit Test
          * Frontend Implement, Frontend FixBug, Frontend Unit Test
          * Frontend Responsive
          * Testing Implement (QA manual)
        - Vi·ªác breakdown n·ªôi b·ªô theo role CH·ªà ƒë·ªÉ estimation ch√≠nh x√°c
        - Export file th√¨ 1 task = 1 row v·ªõi effort columns cho t·∫•t c·∫£ roles

        Nhi·ªám v·ª• c·ªßa b·∫°n:
        1. S·ª≠ d·ª•ng th√¥ng tin t·ª´ GraphRAG ƒë·ªÉ hi·ªÉu s√¢u v·ªÅ requirements
        2. Break down category ƒë∆∞·ª£c giao th√†nh FUNCTIONAL tasks theo business logic
        3. T·∫°o description chi ti·∫øt bao g·ªìm C·∫¢ 3 KH√çA C·∫†NH (Backend, Frontend, Testing)
        4. X√°c ƒë·ªãnh dependencies v√† priority
        5. T·∫°o sub_no theo pattern: X.Y (X=category number, Y=task number trong category)
        6. üö® QUAN TR·ªåNG: Task n√™n c√≥ k√≠ch th∆∞·ªõc v·ª´a ph·∫£i (total effort ~3-10 mandays cho t·∫•t c·∫£ roles)

        TASK SIZE GUIDELINES (total effort across all roles):
        ======================================================
        - Small tasks: 2-5 mandays total - Simple CRUD, basic UI, straightforward testing
        - Medium tasks: 5-10 mandays total - Moderate business logic, integration, comprehensive testing
        - Large tasks: 10-15 mandays total - Complex features, multiple integrations, extensive testing
        - ‚ö†Ô∏è N·∫øu task qu√° l·ªõn (>15 mandays) ‚Üí CHIA NH·ªé th√†nh nhi·ªÅu tasks

        üî• BUSINESS LOGIC BREAKDOWN EXAMPLES:
        ======================================
        
        üìù EXAMPLE 1: User Login Feature
        ---------------------------------
        ‚úÖ TASK 1.1:
           - sub_no: "1.1"
           - task_name: "User Login Feature"
           - parent_task: "" (or "Authentication System" if hierarchical)
           - category: "Authentication & Authorization"
           - description: "Complete user login functionality including:
             
             BACKEND: Implement POST /api/auth/login endpoint with email/password validation, 
             JWT token generation, user authentication, error handling, rate limiting.
             Database queries for user lookup, session management.
             
             FRONTEND: Create login form component with email/password inputs, client-side validation, 
             error message display, loading states, remember me functionality, forgot password link.
             Responsive design for mobile/tablet/desktop.
             
             TESTING: Manual test cases covering: valid credentials login, invalid email/password, 
             empty fields, SQL injection attempts, XSS prevention, session timeout, 
             concurrent login attempts, remember me functionality (10-12 test cases)."
           - complexity: "Medium"
           - priority: "High"
           - premise: "Database schema exists, JWT library available, design mockups ready"
           - remark: "Use bcrypt for password hashing, JWT for authentication"
           - note: "Include rate limiting to prevent brute force attacks"
        
        üìù EXAMPLE 2: Product Search & Filtering
        ------------------------------------------
        ‚úÖ TASK 2.1:
           - sub_no: "2.1"
           - task_name: "Product Search & Filtering"
           - category: "Product Management"
           - description: "Complete product search functionality with advanced filtering:
             
             BACKEND: Create GET /api/products/search endpoint accepting query params 
             (keyword, category, price range, brand, rating). Implement full-text search, 
             filter logic, database query optimization with indexes, pagination, sorting options.
             
             FRONTEND: Build search bar component, filter sidebar with dropdowns/checkboxes 
             for categories/price/brand/rating, product grid display, pagination controls, 
             sort dropdown, loading skeleton, empty state handling, clear filters button.
             
             TESTING: Test cases for keyword search accuracy, filter combinations, 
             price range validation, empty results handling, pagination navigation, 
             special characters in search, performance with large datasets (15-20 test cases)."
           - complexity: "High"
           - priority: "High"

        üìù EXAMPLE 3: Order Checkout Process
        -------------------------------------
        ‚úÖ TASK 3.1:
           - sub_no: "3.1"
           - task_name: "Order Checkout & Payment"
           - category: "Order Management"
           - description: "Complete checkout flow from cart to order confirmation:
             
             BACKEND: Implement POST /api/orders/checkout endpoint with cart validation,
             inventory check, price calculation, tax/shipping calculation, payment gateway integration,
             order creation, inventory update, email notification trigger.
             
             FRONTEND: Multi-step checkout form (shipping address, payment method, order review),
             form validation, payment integration UI, order summary, loading states, 
             error handling, success confirmation page, order receipt display.
             
             TESTING: Test checkout with various payment methods, address validation,
             cart changes during checkout, payment failures, timeout scenarios,
             inventory conflicts, email notifications (20-25 test cases)."
           - complexity: "High"

        üéØ NGUY√äN T·∫ÆC BREAKDOWN:
        ========================
        1. ‚úÖ Identify FUNCTIONAL/BUSINESS REQUIREMENTS (Login, Search, Checkout, etc.)
        2. ‚úÖ Create ONE task per functional requirement
        3. ‚úÖ Task name = Business logic name (NOT "Create API" or "Build UI")
        4. ‚úÖ Description bao g·ªìm ƒê·∫¶Y ƒê·ª¶ 3 kh√≠a c·∫°nh: Backend, Frontend, Testing
        5. ‚úÖ M·ªói kh√≠a c·∫°nh ph·∫£i C·ª§ TH·ªÇ, CHI TI·∫æT ƒë·ªÉ Estimation Worker c√≥ th·ªÉ ∆∞·ªõc l∆∞·ª£ng ch√≠nh x√°c
        6. ‚úÖ X√°c ƒë·ªãnh dependencies gi·ªØa c√°c tasks (dependencies gi·ªØa functional tasks, kh√¥ng ph·∫£i BE/FE/QA)
        7. ‚úÖ ∆Øu ti√™n critical path v√† high-value features
        8. ‚úÖ Task size reasonable: total effort cho t·∫•t c·∫£ roles ~2-15 mandays

        üìã SUB_NO NUMBERING PATTERN:
        ============================
        Format: X.Y
        - X = Category number (1, 2, 3, ...)
        - Y = Task number within category (1, 2, 3, ...)
        
        Example:
        - 1.1 = Category 1, Task 1 (User Login)
        - 1.2 = Category 1, Task 2 (Password Reset)
        - 2.1 = Category 2, Task 1 (Product List)
        - 2.2 = Category 2, Task 2 (Product Detail)

        üì§ OUTPUT FORMAT (JSON):
        ========================
        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi format:
        {
            "breakdown": [
                {
                    "id": "unique_task_id_001",
                    "category": "Authentication & Authorization",
                    "sub_no": "1.1",
                    "parent_task": "",
                    "task_name": "User Login Feature",
                    "description": "Complete user login functionality including:\\n\\nBACKEND: Implement POST /api/auth/login endpoint with email/password validation, JWT token generation, user authentication, error handling.\\n\\nFRONTEND: Create login form component with email/password inputs, client-side validation, error message display, loading states.\\n\\nTESTING: Manual test cases covering valid credentials login, invalid email/password, empty fields, SQL injection attempts (10-12 test cases).",
                    "complexity": "Medium",
                    "dependencies": [],
                    "priority": "High",
                    "premise": "Database schema exists, JWT library available",
                    "remark": "Use bcrypt for password hashing",
                    "note": "Include rate limiting"
                },
                {
                    "id": "unique_task_id_002",
                    "category": "Authentication & Authorization",
                    "sub_no": "1.2",
                    "parent_task": "",
                    "task_name": "Password Reset Flow",
                    "description": "Complete password reset functionality with email verification.",
                    "complexity": "Medium",
                    "dependencies": ["unique_task_id_001"],
                    "priority": "Medium",
                    "premise": "Email service configured",
                    "remark": "Token expiry: 1 hour",
                    "note": "Rate limit reset requests"
                }
            ]
        }
        
        üö® CRITICAL JSON FORMAT REQUIREMENTS:
        ======================================
        1. Return ONLY valid, parseable JSON - no markdown, no explanations
        2. Wrap your response in ```json code blocks for clarity
        3. All newlines in strings MUST be escaped as \\n (not actual line breaks)
        4. All tabs MUST be escaped as \\t
        5. All quotes in strings MUST be escaped as \\"
        6. Description field: Use \\n\\n to separate BACKEND, FRONTEND, TESTING sections
        7. Keep descriptions concise but comprehensive (200-400 chars recommended)
        8. Test JSON validity before returning
        
        ‚ö†Ô∏è CONTENT REQUIREMENTS: 
        - Task name = Business logic (NOT technical role)
        - Description MUST have 3 sections: BACKEND, FRONTEND, TESTING (separated by \\n\\n)
        - Each section must be SPECIFIC and DETAILED for accurate estimation
        - Dependencies are between functional tasks, NOT between BE/FE/QA subtasks
        
        Example of properly escaped description:
        "description": "User authentication feature.\\n\\nBACKEND: Create login API endpoint, JWT generation, password validation.\\n\\nFRONTEND: Build login form, validation, error handling.\\n\\nTESTING: Test valid/invalid credentials, security edge cases (8 test cases)."
        """

    def get_estimation_worker_prompt(self) -> str:
        return """
        B·∫°n l√† m·ªôt Senior Developer v·ªõi 8 nƒÉm kinh nghi·ªám, chuy√™n gia trong vi·ªác estimation effort cho c√°c d·ª± √°n ph·∫ßn m·ªÅm.

        üéØ COMPREHENSIVE MULTI-ROLE ESTIMATION:
        ========================================
        M·ªói task ƒë·∫°i di·ªán cho m·ªôt FUNCTIONAL REQUIREMENT ho√†n ch·ªânh.
        Task description bao g·ªìm requirements cho T·∫§T C·∫¢ roles: Backend, Frontend, v√† Testing.
        
        Nhi·ªám v·ª• c·ªßa b·∫°n:
        1. ƒê·ªåC K·ª∏ description ƒë·ªÉ hi·ªÉu requirements cho T·ª™NG ROLE (Backend, Frontend, Testing)
        2. ESTIMATE EFFORT cho M·ªñI ROLE d·ª±a tr√™n middle developer (3 nƒÉm kinh nghi·ªám)
        3. PH√ÇN CHIA effort theo task types:
           - Backend: Implement, FixBug, UnitTest
           - Frontend: Implement, FixBug, UnitTest, Responsive (n·∫øu c√≥)
           - Testing: Implement (manual test cases)
        4. T√çNH T·ªîNG estimation_manday = sum of all role efforts
        5. X√°c ƒë·ªãnh confidence level d·ª±a tr√™n ƒë·ªô r√µ r√†ng c·ªßa requirements
        6. ‚ö†Ô∏è QUAN TR·ªåNG: Provide CONSERVATIVE estimates - prefer slightly higher estimates for safety
        7. Consider risks, dependencies, and complexity for realistic estimates

        üìä ROLE-SPECIFIC ESTIMATION GUIDELINES:
        ========================================

        üî∑ BACKEND TASKS (role="Backend"):
        -----------------------------------
        Ti√™u chu·∫©n cho middle backend developer (3 nƒÉm kinh nghi·ªám):
        
        API Development:
        - Simple CRUD API (1 resource): 0.5-1 manday
          * 0.3-0.6 manday implementation (endpoints, validation)
          * 0.1-0.2 manday bug fixing & refinement
          * 0.1-0.2 manday unit tests
        
        - Complex API with business logic: 1-2.5 mandays
          * 0.6-1.5 manday implementation (logic, validation, error handling)
          * 0.2-0.5 manday bug fixing
          * 0.2-0.5 manday unit tests
        
        Database Work:
        - Simple schema design (2-3 tables): 0.5-1 manday
        - Migration scripts: 0.2-0.5 manday
        - Query optimization: 0.5-1.5 manday
        
        Authentication/Authorization:
        - JWT implementation: 1-1.5 manday
        - Role-based access control: 1-2 manday
        - OAuth integration: 1.5-2.5 manday

        üî∂ FRONTEND TASKS (role="Frontend"):
        -------------------------------------
        Ti√™u chu·∫©n cho middle frontend developer (3 nƒÉm kinh nghi·ªám):
        
        UI Components:
        - Simple form (2-3 inputs): 0.5-1 manday
          * 0.3-0.6 manday implementation (UI, validation, state)
          * 0.1-0.2 manday bug fixing & polish
          * 0.1-0.2 manday unit tests
        
        - Complex form with multiple steps: 1.5-2.5 mandays
          * 0.9-1.5 manday implementation
          * 0.3-0.5 manday bug fixing
          * 0.3-0.5 manday unit tests
        
        - Data table with filtering: 1-2 mandays
        - Dashboard with charts: 1.5-2.5 mandays
        - Responsive layout (mobile + desktop): +0.5-1 manday
        
        API Integration:
        - Simple GET/POST calls: 0.3-0.5 manday
        - Complex state management with API: 0.8-1.5 manday
        - Real-time updates (WebSocket): 1-2 manday

        üî∏ QA TASKS (role="QA"):
        ------------------------
        Ti√™u chu·∫©n cho QA engineer - MANUAL TESTING (NOT automation):
        
        Test Case Design & Execution:
        - Base estimation formula: 0.1 manday per test case (includes design, execution, documentation)
        
        Estimation based on COMPLEXITY and INPUT COUNT:
        
        Simple Feature (5-8 test cases): 0.5-0.8 manday
        - Example: Login form (valid, invalid, empty, SQL injection, etc.)
        - 5-8 scenarios √ó 0.1 = 0.5-0.8 manday
        
        Medium Feature (10-15 test cases): 1-1.5 mandays
        - Example: Product search with filters
        - Multiple filter combinations, edge cases
        - 10-15 scenarios √ó 0.1 = 1-1.5 manday
        
        Complex Feature (20-30 test cases): 2-3 mandays
        - Example: Checkout flow with payment
        - Multiple payment methods, edge cases, error scenarios
        - 20-30 scenarios √ó 0.1 = 2-3 mandays
        
        Test Case Calculation Factors:
        - Number of input fields: Each field adds 2-3 test cases (valid, invalid, boundary)
        - Business logic branches: Each condition adds 1-2 test cases
        - Integration points: Each integration adds 2-3 test cases
        - Error scenarios: Add 3-5 test cases for error handling
        
        QA Task Breakdown (NO FixBug/UnitTest for QA role):
        - testing_implement: 100% of estimation (all test case design & execution)
        - backend/frontend fields: 0.0 (QA doesn't do implementation)

        ‚ö†Ô∏è CONSERVATIVE ESTIMATION PHILOSOPHY:
        =======================================
        - Prefer realistic/slightly higher estimates over optimistic ones
        - Account for code review, testing, and debugging time
        - Consider integration complexity and potential blockers
        - Include buffer for unexpected issues (built into base estimates)
        - Better to overestimate slightly than underestimate significantly

        Factors ·∫£nh h∆∞·ªüng ƒë·∫øn estimation:
        - Complexity: Low (baseline), Medium (+15%), High (+30%)
        - Dependencies: Nhi·ªÅu dependencies (+15-25%)
        - Risk level: High risk (+20-40%)
        - Unknown technology: Add 20-30% learning buffer
        - External dependencies: Add 15-25% coordination buffer

        üìã TASK TYPE BREAKDOWN BY ROLE:
        ================================
        
        Backend/Frontend Tasks:
        - Implement: 60-65% (core development)
        - FixBug: 15-20% (bug fixing & refinement)
        - UnitTest: 20-25% (unit testing)
        
        QA Tasks:
        - testing_implement: 100% (all test work)
        - NO backend/frontend implement/fixbug/unittest
        
        Example Backend Task (2.0 mandays total):
        - backend_implement: 1.2 manday (60%)
        - backend_fixbug: 0.4 manday (20%)
        - backend_unittest: 0.4 manday (20%)
        - All other fields: 0.0
        
        Example Frontend Task (1.5 mandays total):
        - frontend_implement: 0.9 manday (60%)
        - frontend_fixbug: 0.3 manday (20%)
        - frontend_unittest: 0.3 manday (20%)
        - All other fields: 0.0
        
        Example QA Task (1.2 mandays total):
        - testing_implement: 1.2 manday (100%)
        - All other fields: 0.0
        
        üì§ OUTPUT FORMAT - COMPREHENSIVE ESTIMATION:
        =================================================
        Estimate cho T·∫§T C·∫¢ roles trong 1 response duy nh·∫•t:
        
        {
            "estimation": {
                "id": "task_id",
                "estimation_manday": 5.2,  // T·ªîNG c·ªßa t·∫•t c·∫£ roles
                
                // Backend efforts
                "backend_implement": 1.2,
                "backend_fixbug": 0.4,
                "backend_unittest": 0.4,
                
                // Frontend efforts  
                "frontend_implement": 1.5,
                "frontend_fixbug": 0.5,
                "frontend_unittest": 0.5,
                "responsive_implement": 0.3,
                
                // Testing efforts
                "testing_implement": 0.4,
                
                "confidence_level": 0.8,
                
                "breakdown_reasoning": {
                    "backend": "API endpoint with validation, JWT, error handling. Total: 2.0 mandays (60% implement, 20% fixbug, 20% unittest)",
                    "frontend": "Login form with validation, error messages, loading states. Total: 2.5 mandays including 0.3 responsive",
                    "testing": "4 test cases √ó 0.1 = 0.4 mandays (valid login, invalid, empty, security)"
                },
                
                "test_case_count": 4,
                
                "risk_factors": ["JWT library integration", "Session management complexity"],
                
                "assumptions": [
                    "Database schema exists",
                    "Design mockups available",
                    "Test environment configured"
                ]
            }
        }
        
        ‚ö†Ô∏è CRITICAL RULES:
        ==================
        1. ‚úÖ PH·∫¢I estimate cho T·∫§T C·∫¢ 3 roles (Backend, Frontend, Testing)
        2. ‚úÖ ƒê·ªçc k·ªπ description ƒë·ªÉ t√¨m requirements cho t·ª´ng role
        3. ‚úÖ Backend/Frontend: Split 60% implement, 20% fixbug, 20% unittest
        4. ‚úÖ Testing: 100% testing_implement (0.1 manday √ó s·ªë test cases)
        5. ‚úÖ estimation_manday = SUM of all role fields
        6. ‚úÖ N·∫øu task kh√¥ng c·∫ßn m·ªôt role n√†o ƒë√≥ ‚Üí effort cho role ƒë√≥ = 0
        7. ‚úÖ Provide detailed reasoning cho m·ªói role
        8. ‚úÖ Conservative estimation: prefer slightly higher than lower
        
        üö® CRITICAL JSON FORMAT REQUIREMENTS:
        ======================================
        1. Return ONLY valid, parseable JSON - no markdown explanations before/after
        2. Wrap your response in ```json code blocks for clarity
        3. ALL property names MUST be enclosed in double quotes
        4. NO trailing commas in arrays or objects
        5. NO comments in JSON (// or /* */)
        6. All string values with newlines must use \\n escape sequence
        7. All numeric values must be valid numbers (use 0.0 for zero, not null)
        8. Test JSON validity before returning
        
        Example of properly formatted JSON:
        ```json
        {
            "estimation": {
                "id": "task_123",
                "estimation_manday": 5.2,
                "backend_implement": 1.2,
                "backend_fixbug": 0.4,
                "backend_unittest": 0.4,
                "frontend_implement": 1.5,
                "frontend_fixbug": 0.5,
                "frontend_unittest": 0.5,
                "responsive_implement": 0.3,
                "testing_implement": 0.4,
                "confidence_level": 0.8,
                "breakdown_reasoning": {
                    "backend": "API implementation details",
                    "frontend": "UI implementation details",
                    "testing": "Test case details"
                },
                "test_case_count": 4,
                "risk_factors": ["Factor 1", "Factor 2"],
                "assumptions": ["Assumption 1", "Assumption 2"]
            }
        }
        ```
        
        ‚ö†Ô∏è COMMON JSON ERRORS TO AVOID:
        - Unquoted property names: {estimation_manday: 5.2} ‚ùå ‚Üí {"estimation_manday": 5.2} ‚úÖ
        - Single quotes: {'estimation': {}} ‚ùå ‚Üí {"estimation": {}} ‚úÖ
        - Trailing commas: {"a": 1, "b": 2,} ‚ùå ‚Üí {"a": 1, "b": 2} ‚úÖ
        - Comments: {/* comment */ "a": 1} ‚ùå ‚Üí {"a": 1} ‚úÖ
        """

    def get_validation_worker_prompt(self) -> str:
        return """
        B·∫°n l√† m·ªôt Project Manager v·ªõi chuy√™n m√¥n s√¢u v·ªÅ quality assurance v√† risk management.

        Nhi·ªám v·ª• c·ªßa b·∫°n:
        1. Validate c√°c estimations t·ª´ Estimation Worker
        2. Cross-check logic v√† consistency
        3. √Åp d·ª•ng buffer cho risk mitigation
        4. ƒê·∫£m b·∫£o total effort h·ª£p l√Ω

        Validation criteria:
        - Consistency check: So s√°nh v·ªõi c√°c task t∆∞∆°ng t·ª±
        - Dependency validation: ƒê·∫£m b·∫£o dependencies ƒë∆∞·ª£c t√≠nh ƒë√∫ng
        - Risk assessment: ƒê√°nh gi√° v√† apply buffer cho high-risk tasks
        - Team capacity: Xem x√©t realistic capacity c·ªßa team
        - Buffer calculation: 10-20% cho c√°c task c√≥ risk

        Adjustment rules:
        - Low complexity, low risk: Kh√¥ng adjust
        - Medium complexity/risk: +10% buffer
        - High complexity/risk: +20% buffer
        - Critical path tasks: +15% buffer
        - New technology/framework: +25% buffer

        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi format:
        {
            "validation": {
                "id": "task_id",
                "original_estimation": 2.5,
                "validated_estimation": 2.8,
                "adjustment_reason": "Added 15% buffer for dependency risk",
                "confidence_level": 0.85,
                "validation_notes": "Estimation appears reasonable, added buffer for external dependencies",
                "risk_mitigation": ["parallel development of dependent tasks", "early API integration testing"]
            }
        }
        
        üö® CRITICAL JSON FORMAT REQUIREMENTS:
        ======================================
        1. Return ONLY valid, parseable JSON - no markdown explanations before/after
        2. Wrap your response in ```json code blocks for clarity
        3. ALL property names MUST be enclosed in double quotes
        4. NO trailing commas in arrays or objects
        5. NO comments in JSON (// or /* */)
        6. All string values must properly escape special characters
        7. All numeric values must be valid numbers (use 0.0 for zero, not null)
        8. Test JSON validity before returning
        
        ‚ö†Ô∏è COMMON JSON ERRORS TO AVOID:
        - Unquoted property names: {validated_estimation: 2.8} ‚ùå ‚Üí {"validated_estimation": 2.8} ‚úÖ
        - Single quotes: {'validation': {}} ‚ùå ‚Üí {"validation": {}} ‚úÖ
        - Trailing commas: {"a": 1, "b": 2,} ‚ùå ‚Üí {"a": 1, "b": 2} ‚úÖ
        - Comments: {/* comment */ "a": 1} ‚ùå ‚Üí {"a": 1} ‚úÖ
        """

# ========================
# Enhanced Orchestrator Node
# ========================

def enhanced_orchestrator_node(state: EnhancedOrchestratorState) -> Dict[str, Any]:
    """
    Enhanced Orchestrator v·ªõi GraphRAG integration
    """
    logger.info(f"üéØ Enhanced Orchestrator ƒëang ph√¢n t√≠ch task: {state['original_task']}")

    llm_handler = EnhancedEstimationLLM()

    # S·ª≠ d·ª•ng pre-fetched GraphRAG insights t·ª´ state
    graphrag_insights = state.get('graphrag_insights', [])
    if graphrag_insights:
        logger.info(f"üìä ƒêang s·ª≠ d·ª•ng {len(graphrag_insights)} GraphRAG insights c√≥ s·∫µn...")
    else:
        logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ GraphRAG insights, s·ª≠ d·ª•ng analysis c∆° b·∫£n")

    # T·∫°o context t·ª´ GraphRAG insights
    graphrag_context = ""
    if graphrag_insights:
        graphrag_context = "\n\nContext t·ª´ GraphRAG:\n"
        for insight in graphrag_insights:
            graphrag_context += f"Q: {insight['query']}\nA: {insight['response']}\n---\n"

    # T·∫°o prompt cho Orchestrator
    messages = [
        SystemMessage(content=llm_handler.get_orchestrator_prompt()),
        HumanMessage(content=f"""
        Task c·∫ßn ph√¢n t√≠ch v√† estimation:
        {state['original_task']}

        {graphrag_context}

        D·ª±a tr√™n task v√† context t·ª´ GraphRAG, h√£y ph√¢n t√≠ch v√† ƒë∆∞a ra chi·∫øn l∆∞·ª£c breakdown.
        """)
    ]

    try:
        response = llm_handler.llm.invoke(messages)

        # üÜï LOG RAW LLM RESPONSE
        logger.info(f"ü§ñ [ORCHESTRATOR] LLM Raw Response:")
        logger.debug(f"   Content Length: {len(response.content)} chars")
        logger.debug(f"   Raw Content:\n{response.content}")
        
        # Parse JSON response
        import re
        json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            
            # üÜï LOG PARSED RESULT
            logger.info(f"‚úÖ [ORCHESTRATOR] Parsed JSON successfully")
            logger.debug(f"   Parsed Result: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            # üÜï DUMP TO FILE
            dump_llm_response(
                worker_name='orchestrator',
                task_info=state['original_task'][:100],
                raw_response=response.content,
                parsed_result=result,
                project_id=state.get('project_id')
            )

            categories = result.get('categories', [])

            logger.info(f"‚úÖ Orchestrator ƒë√£ ph√¢n t√≠ch: {len(categories)} categories")
            logger.info(f"üìà Complexity: {result.get('complexity_assessment', 'Unknown')}")

            return {
                'main_categories': categories,
                'graphrag_insights': graphrag_insights,
                'workflow_status': 'orchestrator_completed'
            }
        else:
            logger.error(f"‚ùå [ORCHESTRATOR] Failed to find JSON in response")
            raise ValueError("Kh√¥ng th·ªÉ parse JSON response t·ª´ Orchestrator")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong Enhanced Orchestrator: {e}")
        return {
            'main_categories': [],
            'graphrag_insights': graphrag_insights,
            'workflow_status': 'orchestrator_failed'
        }

# ========================
# Worker 1: Task Breakdown v·ªõi GraphRAG
# ========================

def task_breakdown_worker(worker_input) -> Dict[str, Any]:
    """
    Worker 1: Chuy√™n break down task v·ªõi GraphRAG integration
    Receives data via Send() mechanism
    """
    # Extract data from worker input
    category_focus = worker_input.get('category_focus', 'General')
    original_task = worker_input.get('original_task', '')

    logger.info(f"üë∑‚Äç‚ôÇÔ∏è Worker 1 (Task Breakdown) ƒëang x·ª≠ l√Ω category: {category_focus}")

    llm_handler = EnhancedEstimationLLM()

    # Note: GraphRAG insights are already available in the orchestrator state
    # and used for overall project understanding. No additional GraphRAG calls needed here.
    category_context = f"\nCategory focus: {category_focus}\n"

    messages = [
        SystemMessage(content=llm_handler.get_breakdown_worker_prompt()),
        HumanMessage(content=f"""
        Original Task: {original_task}
        Category Focus: {category_focus}

        {category_context}

        H√£y break down category '{category_focus}' th√†nh c√°c task c·ª• th·ªÉ v·ªõi description chi ti·∫øt.
        """)
    ]

    try:
        response = llm_handler.llm.invoke(messages)

        # üÜï LOG RAW LLM RESPONSE
        logger.info(f"ü§ñ [BREAKDOWN_WORKER] LLM Raw Response for category: {category_focus}")
        logger.debug(f"   Content Length: {len(response.content)} chars")
        logger.debug(f"   Raw Content:\n{response.content}")

        # Parse JSON response with markdown code block handling
        import re
        raw_response = response.content
        
        # Extract JSON from markdown code blocks if present
        if "```json" in raw_response:
            json_match = re.search(r'```json\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if json_match:
                raw_response = json_match.group(1)
                logger.debug("   Extracted JSON from ```json code block")
        elif "```" in raw_response:
            json_match = re.search(r'```\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if json_match:
                raw_response = json_match.group(1)
                logger.debug("   Extracted JSON from ``` code block")
        
        # Try to find JSON object
        json_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                result = json.loads(json_str)
            except json.JSONDecodeError as json_err:
                logger.error(f"   JSON parsing failed: {json_err}")
                logger.error(f"   Attempted to parse: {json_str[:500]}...")
                raise ValueError(f"Invalid JSON from Breakdown Worker: {json_err}")
            
            # üÜï LOG PARSED RESULT
            logger.info(f"‚úÖ [BREAKDOWN_WORKER] Parsed JSON successfully for: {category_focus}")
            logger.debug(f"   Parsed Result: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            # üÜï DUMP TO FILE
            project_id = worker_input.get('project_id')
            dump_llm_response(
                worker_name='breakdown_worker',
                task_info=f"Category: {category_focus}",
                raw_response=response.content,
                parsed_result=result,
                project_id=project_id
            )
            
            breakdown_tasks = result.get('breakdown', [])

            # POST-PROCESSING VALIDATION: Business logic task validation
            validated_tasks = []
            oversized_tasks = []
            
            for task in breakdown_tasks:
                task['worker_source'] = 'task_breakdown_worker'
                task['confidence_level'] = 0.8  # Default confidence t·ª´ breakdown

                # Validate task structure
                complexity = task.get('complexity', 'Medium')
                description = task.get('description', '')
                task_name = task.get('task_name', '')

                # Check if description contains all 3 sections (Backend, Frontend, Testing)
                has_backend = 'backend' in description.lower() or 'api' in description.lower()
                has_frontend = 'frontend' in description.lower() or 'ui' in description.lower()
                has_testing = 'testing' in description.lower() or 'test case' in description.lower()
                
                if not (has_backend and has_frontend and has_testing):
                    logger.warning(f"‚ö†Ô∏è Task '{task_name}' missing role sections in description:")
                    if not has_backend:
                        logger.warning(f"   - Missing BACKEND section")
                    if not has_frontend:
                        logger.warning(f"   - Missing FRONTEND section")
                    if not has_testing:
                        logger.warning(f"   - Missing TESTING section")

                # Heuristic check for reasonable task size (total effort ~3-15 mandays)
                is_potentially_oversized = False

                # Check 1: High complexity with very broad scope
                if complexity == 'High' and any(keyword in description.lower() or keyword in task_name.lower()
                    for keyword in ['entire system', 'complete platform', 'full stack', 'all features', 
                                    'to√†n b·ªô h·ªá th·ªëng', 'ho√†n ch·ªânh']):
                    is_potentially_oversized = True

                # Check 2: Description extremely long (>600 chars suggests overly complex)
                if len(description) > 600:
                    is_potentially_oversized = True
                    logger.warning(f"‚ö†Ô∏è Task '{task_name}' has very long description ({len(description)} chars)")

                # Check 3: Too many major components (suggests task should be split)
                component_keywords = ['database', 'api', 'microservice', 'authentication', 'authorization', 
                                      'payment', 'notification', 'reporting', 'analytics', 'dashboard']
                component_count = sum(1 for keyword in component_keywords if keyword in description.lower())
                if component_count > 5:
                    is_potentially_oversized = True
                    logger.warning(f"‚ö†Ô∏è Task '{task_name}' mentions {component_count} major components")

                if is_potentially_oversized:
                    oversized_tasks.append(task)
                    logger.warning(f"‚ö†Ô∏è Potentially oversized task: '{task_name}' (complexity: {complexity})")
                else:
                    validated_tasks.append(task)

            # If oversized tasks found, log warning but still include them
            # (Let estimation worker handle the actual effort calculation)
            if oversized_tasks:
                logger.warning(f"‚ö†Ô∏è {len(oversized_tasks)} potentially oversized tasks detected.")
                logger.warning(f"   These tasks may have very high total effort (>15 mandays). Consider splitting.")
                logger.warning(f"   Tasks: {[t.get('task_name', 'Unknown') for t in oversized_tasks]}")
                # Still add them to results for estimation worker to process
                validated_tasks.extend(oversized_tasks)

            logger.info(f"‚úÖ Worker 1 completed: {len(validated_tasks)} tasks cho {category_focus}")
            if oversized_tasks:
                logger.info(f"   - {len(oversized_tasks)} tasks may need review/splitting")

            return {
                'breakdown_results': validated_tasks
            }
        else:
            raise ValueError("Kh√¥ng th·ªÉ parse JSON response t·ª´ Breakdown Worker")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong Task Breakdown Worker: {e}")
        return {
            'breakdown_results': []
        }

# ========================
# Worker 2: Estimation Worker
# ========================

def calculate_smart_buffer(task_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calculate intelligent buffer based on risk factors, complexity, and dependencies.
    Returns buffer info with percentage and reasoning.
    """
    buffer = 0.0
    reasons = []

    # Complexity-based buffer
    complexity = task_data.get('complexity', 'Medium')
    if complexity == 'High':
        buffer += 0.20
        reasons.append("High complexity (+20%)")
    elif complexity == 'Medium':
        buffer += 0.10
        reasons.append("Medium complexity (+10%)")

    # Risk factors buffer
    risk_factors = task_data.get('risk_factors', [])
    if len(risk_factors) > 2:
        buffer += 0.15
        reasons.append(f"Multiple risk factors ({len(risk_factors)}) (+15%)")
    elif len(risk_factors) > 0:
        buffer += 0.08
        reasons.append(f"Some risk factors ({len(risk_factors)}) (+8%)")

    # Dependencies buffer
    dependencies = task_data.get('dependencies', [])
    if len(dependencies) > 3:
        buffer += 0.12
        reasons.append(f"Many dependencies ({len(dependencies)}) (+12%)")
    elif len(dependencies) > 0:
        buffer += 0.05
        reasons.append(f"Some dependencies ({len(dependencies)}) (+5%)")

    # Priority-based buffer for critical tasks
    priority = task_data.get('priority', 'Medium')
    if priority == 'High':
        buffer += 0.10
        reasons.append("High priority task (+10%)")

    # Cap maximum buffer at 50%
    buffer = min(buffer, 0.50)

    return {
        'buffer_percentage': buffer,
        'buffer_reasons': reasons,
        'adjustment_applied': buffer > 0
    }

def estimation_worker(worker_input) -> Dict[str, Any]:
    """
    Worker 2: Chuy√™n estimation effort cho c√°c task
    Receives task_breakdown via Send() mechanism
    Enhanced with few-shot prompting from historical data
    NOW INCLUDES: Smart buffer calculation and built-in validation (Option 1)
    """
    # Extract task data from worker input
    task_breakdown = worker_input.get('task_breakdown', {})
    task_name = task_breakdown.get('task_name', 'Unknown Task')

    logger.info(f"üë∑‚Äç‚ôÇÔ∏è Worker 2 (Estimation) ƒëang estimate: {task_name}")

    llm_handler = EnhancedEstimationLLM()

    # NEW: Search for similar historical estimations for few-shot prompting
    from utils.estimation_history_manager import get_history_manager

    # Extract project_id from worker_input (passed from state)
    project_id = worker_input.get('project_id', None)

    few_shot_context = ""
    try:
        # Get project-scoped history manager
        history_manager = get_history_manager(project_id=project_id)

        # Create search query from task data
        search_description = task_breakdown.get('description', '')
        search_category = task_breakdown.get('category')
        
        logger.debug(f"   üîç Searching historical data with:")
        if project_id:
            logger.debug(f"     - Project ID: {project_id}")
        logger.debug(f"     - Description: {search_description[:100]}...")
        logger.debug(f"     - Category: {search_category}")

        # Search for similar tasks (no role filter - we now search by business logic)
        similar_tasks = history_manager.search_similar(
            description=search_description,
            category=search_category,
            role=None,  # No role filter in new architecture
            top_k=5,
            similarity_threshold=0.6
        )

        if similar_tasks:
            logger.info(f"   üìö Found {len(similar_tasks)} similar historical tasks")
            few_shot_context = history_manager.build_few_shot_prompt(similar_tasks, max_examples=5)
            
            # Log the few-shot context for debugging
            logger.debug(f"   üìù Few-shot context generated ({len(few_shot_context)} chars):")
            logger.debug(f"   {few_shot_context[:500]}..." if len(few_shot_context) > 500 else f"   {few_shot_context}")
        else:
            logger.debug(f"   ‚ÑπÔ∏è No similar historical tasks found")
            few_shot_context = "No similar historical tasks found. Please estimate based on your expertise."

    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Could not retrieve historical data: {e}")
        few_shot_context = "Historical data unavailable. Please estimate based on your expertise."
    
    # Log the final few-shot context that will be sent to LLM
    logger.debug(f"   üéØ Final few-shot context to be used:")
    if len(few_shot_context) > 200:
        logger.debug(f"   {few_shot_context[:200]}... (truncated, total: {len(few_shot_context)} chars)")
    else:
        logger.debug(f"   {few_shot_context}")

    messages = [
        SystemMessage(content=llm_handler.get_estimation_worker_prompt()),
        HumanMessage(content=f"""
        Task c·∫ßn estimation (FUNCTIONAL/BUSINESS REQUIREMENT):
        - Category: {task_breakdown.get('category', '')}
        - Task Name: {task_breakdown.get('task_name', '')}
        - Parent Task: {task_breakdown.get('parent_task', '')}
        - Description: {task_breakdown.get('description', '')}
        - Complexity: {task_breakdown.get('complexity', 'Medium')}
        - Dependencies: {task_breakdown.get('dependencies', [])}
        - Priority: {task_breakdown.get('priority', 'Medium')}

        üéØ COMPREHENSIVE MULTI-ROLE ESTIMATION REQUIRED:
        =================================================
        Task description bao g·ªìm requirements cho T·∫§T C·∫¢ 3 roles:
        - BACKEND section: API/database/business logic requirements
        - FRONTEND section: UI/components/interactions requirements
        - TESTING section: Test cases/scenarios requirements
        
        B·∫†N PH·∫¢I ESTIMATE EFFORT CHO T·∫§T C·∫¢ 3 ROLES:
        ============================================
        
        1. BACKEND EFFORT:
           - ƒê·ªçc BACKEND section trong description
           - Estimate: backend_implement, backend_fixbug, backend_unittest
           - Split: 60% implement, 20% fixbug, 20% unittest
        
        2. FRONTEND EFFORT:
           - ƒê·ªçc FRONTEND section trong description
           - Estimate: frontend_implement, frontend_fixbug, frontend_unittest, responsive_implement
           - Split: 60% implement, 20% fixbug, 20% unittest
           - Th√™m responsive_implement n·∫øu c√≥ responsive requirements
        
        3. TESTING EFFORT:
           - ƒê·ªçc TESTING section trong description
           - ƒê·∫øm s·ªë test cases c·∫ßn thi·∫øt: valid inputs, invalid inputs, boundary cases, 
             integration tests, error handling
           - testing_implement = s·ªë_test_cases √ó 0.1 manday
        
        4. T·ªîNG EFFORT:
           - estimation_manday = SUM(all backend + all frontend + all testing)
        
        ‚ö†Ô∏è N·∫æU task kh√¥ng c·∫ßn role n√†o ƒë√≥ (v√≠ d·ª•: ch·ªâ backend, kh√¥ng c√≥ UI):
           - ƒê·∫∑t effort cho role ƒë√≥ = 0
           - Gi·∫£i th√≠ch l√Ω do trong reasoning

        {few_shot_context}

        H√£y estimate effort cho middle-level professional (3 nƒÉm kinh nghi·ªám) v·ªõi unit manday (7 gi·ªù/ng√†y).
        S·ª≠ d·ª•ng c√°c historical examples l√†m tham kh·∫£o n·∫øu c√≥.
        Provide detailed reasoning cho T·ª™NG ROLE, gi·∫£i th√≠ch c√°ch t√≠nh test cases cho Testing.
        """)
    ]

    try:
        response = llm_handler.llm.invoke(messages)

        # üÜï LOG RAW LLM RESPONSE
        logger.info(f"ü§ñ [ESTIMATION_WORKER] LLM Raw Response for task: {task_name}")
        logger.debug(f"   Content Length: {len(response.content)} chars")
        logger.debug(f"   Raw Content:\n{response.content}")

        # Parse JSON response with markdown code block handling
        import re
        raw_response = response.content
        
        # Extract JSON from markdown code blocks if present
        if "```json" in raw_response:
            json_match = re.search(r'```json\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if json_match:
                raw_response = json_match.group(1)
                logger.debug("   Extracted JSON from ```json code block")
        elif "```" in raw_response:
            json_match = re.search(r'```\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if json_match:
                raw_response = json_match.group(1)
                logger.debug("   Extracted JSON from ``` code block")
        
        # Try to find JSON object
        json_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                result = json.loads(json_str)
            except json.JSONDecodeError as json_err:
                logger.error(f"   JSON parsing failed: {json_err}")
                logger.error(f"   Error at position {json_err.pos}: {json_err.msg}")
                logger.error(f"   Attempted to parse: {json_str[:500]}...")
                raise ValueError(f"Invalid JSON from Estimation Worker: {json_err}")
            
            # üÜï LOG PARSED RESULT
            logger.info(f"‚úÖ [ESTIMATION_WORKER] Parsed JSON successfully for: {task_name}")
            logger.debug(f"   Parsed Result: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            # üÜï DUMP TO FILE
            project_id = worker_input.get('project_id')
            dump_llm_response(
                worker_name='estimation_worker',
                task_info=f"Task: {task_name}",
                raw_response=response.content,
                parsed_result=result,
                project_id=project_id
            )

            estimation_data = result.get('estimation', {})

            # Merge v·ªõi original task data
            estimated_task = task_breakdown.copy()

            # Extract detailed task type breakdowns
            backend_impl = estimation_data.get('backend_implement', 0.0)
            backend_fix = estimation_data.get('backend_fixbug', 0.0)
            backend_test = estimation_data.get('backend_unittest', 0.0)
            frontend_impl = estimation_data.get('frontend_implement', 0.0)
            frontend_fix = estimation_data.get('frontend_fixbug', 0.0)
            frontend_test = estimation_data.get('frontend_unittest', 0.0)
            responsive_impl = estimation_data.get('responsive_implement', 0.0)
            testing_impl = estimation_data.get('testing_implement', 0.0)

            # Calculate role totals
            estimation_backend = backend_impl + backend_fix + backend_test
            estimation_frontend = frontend_impl + frontend_fix + frontend_test + responsive_impl  # Include responsive in frontend
            estimation_testing = testing_impl
            estimation_infra = 0.0  # Infra not broken down by task type

            # Calculate total estimation
            total_estimation = estimation_backend + estimation_frontend + estimation_testing + estimation_infra

            # If LLM didn't provide detailed breakdown, use total from estimation_manday
            if total_estimation == 0.0:
                total_estimation = estimation_data.get('estimation_manday', 3.0)
                logger.warning(f"   ‚ö†Ô∏è LLM didn't provide role-specific breakdown, using total: {total_estimation}")
                
                # Fallback: distribute equally across all roles if no breakdown provided
                # This should rarely happen if prompt is followed correctly
                backend_impl = total_estimation * 0.3  # 30% to backend
                backend_fix = total_estimation * 0.1
                backend_test = total_estimation * 0.1
                estimation_backend = total_estimation * 0.5
                
                frontend_impl = total_estimation * 0.2  # 20% to frontend
                frontend_fix = total_estimation * 0.05
                frontend_test = total_estimation * 0.05
                estimation_frontend = total_estimation * 0.3
                
                testing_impl = total_estimation * 0.2  # 20% to testing
                estimation_testing = total_estimation * 0.2
                
                logger.warning(f"   Applied fallback distribution: BE={estimation_backend:.1f}, FE={estimation_frontend:.1f}, Testing={estimation_testing:.1f}")

            # OPTION 1: Apply smart buffer calculation with built-in validation
            buffer_info = calculate_smart_buffer(task_breakdown)
            buffer_multiplier = 1.0 + buffer_info['buffer_percentage']

            # Apply buffer to all estimations
            buffered_total = total_estimation * buffer_multiplier
            buffered_backend = estimation_backend * buffer_multiplier
            buffered_frontend = estimation_frontend * buffer_multiplier
            buffered_testing = estimation_testing * buffer_multiplier
            buffered_infra = estimation_infra * buffer_multiplier

            # Apply buffer to detailed breakdowns
            buffered_backend_impl = backend_impl * buffer_multiplier
            buffered_backend_fix = backend_fix * buffer_multiplier
            buffered_backend_test = backend_test * buffer_multiplier
            buffered_frontend_impl = frontend_impl * buffer_multiplier
            buffered_frontend_fix = frontend_fix * buffer_multiplier
            buffered_frontend_test = frontend_test * buffer_multiplier
            buffered_responsive_impl = responsive_impl * buffer_multiplier
            buffered_testing_impl = testing_impl * buffer_multiplier

            estimated_task.update({
                'estimation_manday': buffered_total,
                'estimation_backend_manday': buffered_backend,
                'estimation_frontend_manday': buffered_frontend,
                'estimation_testing_manday': buffered_testing,
                'estimation_infra_manday': buffered_infra,
                'original_estimation': total_estimation,  # Keep original before buffer
                'buffer_applied': buffer_info['buffer_percentage'],
                'buffer_reasons': buffer_info['buffer_reasons'],
                'confidence_level': estimation_data.get('confidence_level', 0.7),
                'estimation_breakdown': estimation_data.get('breakdown', {}),
                'risk_factors': estimation_data.get('risk_factors', []),
                'assumptions': estimation_data.get('assumptions', []),
                'worker_source': 'estimation_worker_with_validation',
                'validation_notes': f"Smart buffer applied: {buffer_info['buffer_percentage']*100:.0f}% - " + ", ".join(buffer_info['buffer_reasons']) if buffer_info['adjustment_applied'] else "No buffer needed",
                # Sun Asterisk detailed breakdown (with buffer)
                'backend_implement': buffered_backend_impl,
                'backend_fixbug': buffered_backend_fix,
                'backend_unittest': buffered_backend_test,
                'frontend_implement': buffered_frontend_impl,
                'frontend_fixbug': buffered_frontend_fix,
                'frontend_unittest': buffered_frontend_test,
                'responsive_implement': buffered_responsive_impl,
                'testing_implement': buffered_testing_impl
            })

            logger.info(f"‚úÖ Worker 2 estimated: {total_estimation:.1f} ‚Üí {buffered_total:.1f} mandays (Buffer: {buffer_info['buffer_percentage']*100:.0f}%)")
            logger.info(f"   - Backend: {buffered_backend:.1f} MD, Frontend: {buffered_frontend:.1f} MD, Testing: {buffered_testing:.1f} MD")

            return {
                'estimation_results': [estimated_task]
            }
        else:
            raise ValueError("Kh√¥ng th·ªÉ parse JSON response t·ª´ Estimation Worker")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong Estimation Worker: {e}")
        # Return task v·ªõi default estimation (balanced across all roles)
        fallback_task = task_breakdown.copy() if task_breakdown else {}
        
        # Default: split effort equally across Backend, Frontend, Testing
        # Total = 3.0 mandays (1.0 per role)
        fallback_task.update({
            'estimation_manday': 3.0,  # Default fallback
            'estimation_backend_manday': 1.0,
            'estimation_frontend_manday': 1.0,
            'estimation_testing_manday': 0.5,
            'estimation_infra_manday': 0.0,
            'original_estimation': 3.0,
            'confidence_level': 0.5,
            'worker_source': 'estimation_worker_fallback',
            # Backend breakdown (60/20/20)
            'backend_implement': 0.6,
            'backend_fixbug': 0.2,
            'backend_unittest': 0.2,
            # Frontend breakdown (60/20/20)
            'frontend_implement': 0.6,
            'frontend_fixbug': 0.2,
            'frontend_unittest': 0.2,
            'responsive_implement': 0.0,
            # Testing (100% implement)
            'testing_implement': 0.5
        })
        logger.warning(f"   Using fallback estimation: 3.0 mandays total (BE: 1.0, FE: 1.0, Testing: 0.5)")
        return {
            'estimation_results': [fallback_task]
        }

# ========================
# OPTION 2: Rule-based Validation (Deterministic)
# ========================

def apply_validation_rules(estimation_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Apply deterministic validation rules to estimation results.
    This is a post-processing step without LLM calls (Option 2).

    Returns updated estimation results with additional validation metadata.
    """
    validated_results = []

    for task in estimation_results:
        # Skip if already has buffer applied (from Option 1)
        if task.get('buffer_applied', 0) > 0:
            validated_results.append(task)
            continue

        # Calculate buffer for tasks without it
        buffer_info = calculate_smart_buffer(task)
        buffer_multiplier = 1.0 + buffer_info['buffer_percentage']

        # Apply buffer to estimation
        original_estimation = task.get('estimation_manday', 0)
        validated_task = task.copy()

        validated_task.update({
            'estimation_manday': original_estimation * buffer_multiplier,
            'original_estimation': original_estimation,
            'buffer_applied': buffer_info['buffer_percentage'],
            'buffer_reasons': buffer_info['buffer_reasons'],
            'validation_notes': f"Rule-based buffer: {buffer_info['buffer_percentage']*100:.0f}% - " + ", ".join(buffer_info['buffer_reasons']) if buffer_info['adjustment_applied'] else "No buffer needed",
            'validation_method': 'deterministic_rules'
        })

        validated_results.append(validated_task)

    return validated_results

# ========================
# OPTION 3: Conditional Validation Filter
# ========================

def should_validate(task: Dict[str, Any]) -> bool:
    """
    Determine if a task needs LLM-based validation (Option 3).
    Only validate high-risk, low-confidence, or critical path tasks.

    Returns True if task requires validation, False otherwise.
    """
    # High-risk tasks (>2 risk factors)
    risk_factors = task.get('risk_factors', [])
    if len(risk_factors) > 2:
        return True

    # Low confidence tasks (<0.6)
    confidence = task.get('confidence_level', 1.0)
    if confidence < 0.6:
        return True

    # Critical path tasks (high priority + many dependencies)
    priority = task.get('priority', 'Medium')
    dependencies = task.get('dependencies', [])
    if priority == 'High' and len(dependencies) > 2:
        return True

    # High complexity with low confidence
    complexity = task.get('complexity', 'Medium')
    if complexity == 'High' and confidence < 0.75:
        return True

    return False

# ========================
# Worker 3: Conditional Validation Worker (Option 3)
# ========================

def validation_worker(worker_input) -> Dict[str, Any]:
    """
    Worker 3: Validation v√† calculation v·ªõi risk mitigation
    Receives estimation_task via Send() mechanism
    """
    # Extract estimation task from worker input
    estimation_task = worker_input.get('estimation_task', {})
    task_name = estimation_task.get('task_name', 'Unknown Task')

    logger.info(f"üë∑‚Äç‚ôÇÔ∏è Worker 3 (Validation) ƒëang validate: {task_name}")

    llm_handler = EnhancedEstimationLLM()

    messages = [
        SystemMessage(content=llm_handler.get_validation_worker_prompt()),
        HumanMessage(content=f"""
        Task c·∫ßn validation:
        - ID: {estimation_task.get('id', '')}
        - Category: {estimation_task.get('category', '')}
        - Task Name: {estimation_task.get('task_name', '')}
        - Description: {estimation_task.get('description', '')}
        - Original Estimation: {estimation_task.get('estimation_manday', 0)} mandays
        - Complexity: {estimation_task.get('complexity', 'Medium')}
        - Dependencies: {estimation_task.get('dependencies', [])}
        - Risk Factors: {estimation_task.get('risk_factors', [])}
        - Confidence Level: {estimation_task.get('confidence_level', 0.7)}

        H√£y validate estimation n√†y v√† apply buffer n·∫øu c·∫ßn thi·∫øt.
        """)
    ]

    try:
        response = llm_handler.llm.invoke(messages)

        # üÜï LOG RAW LLM RESPONSE
        logger.info(f"ü§ñ [VALIDATION_WORKER] LLM Raw Response for task: {task_name}")
        logger.debug(f"   Content Length: {len(response.content)} chars")
        logger.debug(f"   Raw Content:\n{response.content}")

        # Parse JSON response with markdown code block handling
        import re
        raw_response = response.content
        
        # Extract JSON from markdown code blocks if present
        if "```json" in raw_response:
            json_match = re.search(r'```json\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if json_match:
                raw_response = json_match.group(1)
                logger.debug("   Extracted JSON from ```json code block")
        elif "```" in raw_response:
            json_match = re.search(r'```\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if json_match:
                raw_response = json_match.group(1)
                logger.debug("   Extracted JSON from ``` code block")
        
        # Try to find JSON object
        json_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                result = json.loads(json_str)
            except json.JSONDecodeError as json_err:
                logger.error(f"   JSON parsing failed: {json_err}")
                logger.error(f"   Error at position {json_err.pos}: {json_err.msg}")
                logger.error(f"   Attempted to parse: {json_str[:500]}...")
                raise ValueError(f"Invalid JSON from Validation Worker: {json_err}")
            
            # üÜï LOG PARSED RESULT
            logger.info(f"‚úÖ [VALIDATION_WORKER] Parsed JSON successfully for: {task_name}")
            logger.debug(f"   Parsed Result: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            # üÜï DUMP TO FILE
            project_id = worker_input.get('project_id')
            dump_llm_response(
                worker_name='validation_worker',
                task_info=f"Task: {task_name}",
                raw_response=response.content,
                parsed_result=result,
                project_id=project_id
            )
            
            validation_data = result.get('validation', {})

            # Create final validated task
            validated_task = estimation_task.copy()
            
            # Get validated estimation (total)
            validated_estimation = validation_data.get('validated_estimation', estimation_task.get('estimation_manday', 0))
            original_estimation = validation_data.get('original_estimation', estimation_task.get('estimation_manday', 0))
            
            # Calculate adjustment ratio if validation changed the estimation
            adjustment_ratio = 1.0
            if original_estimation > 0:
                adjustment_ratio = validated_estimation / original_estimation
            
            # Apply adjustment ratio to role-specific estimations
            original_backend = estimation_task.get('estimation_backend_manday', 0.0)
            original_frontend = estimation_task.get('estimation_frontend_manday', 0.0)
            original_testing = estimation_task.get('estimation_testing_manday', 0.0)
            original_infra = estimation_task.get('estimation_infra_manday', 0.0)

            validated_task.update({
                'estimation_manday': validated_estimation,
                'estimation_backend_manday': original_backend * adjustment_ratio,
                'estimation_frontend_manday': original_frontend * adjustment_ratio,
                'estimation_testing_manday': original_testing * adjustment_ratio,
                'estimation_infra_manday': original_infra * adjustment_ratio,
                'original_estimation': original_estimation,
                'confidence_level': validation_data.get('confidence_level', estimation_task.get('confidence_level', 0.7)),
                'validation_notes': validation_data.get('validation_notes', ''),
                'adjustment_reason': validation_data.get('adjustment_reason', ''),
                'risk_mitigation': validation_data.get('risk_mitigation', []),
                'worker_source': 'validation_worker'
            })

            logger.info(f"‚úÖ Worker 3 validated: {original_estimation:.1f} ‚Üí {validated_estimation:.1f} mandays")

            return {
                'validated_results': [validated_task]
            }
        else:
            raise ValueError("Kh√¥ng th·ªÉ parse JSON response t·ª´ Validation Worker")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong Validation Worker: {e}")
        # Return task v·ªõi minimal validation
        fallback_task = estimation_task.copy() if estimation_task else {}
        fallback_task.update({
            'validation_notes': f'Validation failed, using original estimation: {e}',
            'worker_source': 'validation_worker_fallback'
        })
        return {
            'validated_results': [fallback_task]
        }

# ========================
# Assignment Functions
# ========================

def assign_breakdown_workers(state: EnhancedOrchestratorState) -> List[Send]:
    """
    Ph√¢n c√¥ng breakdown workers cho m·ªói category
    """
    categories = state.get('main_categories', [])
    project_id = state.get('project_id', None)
    logger.info(f"üìã ƒêang ph√¢n c√¥ng breakdown workers cho {len(categories)} categories")

    sends = []
    for category in categories:
        send = Send(
            "task_breakdown_worker",
            {
                "category_focus": category,
                "original_task": state['original_task'],
                "project_id": project_id  # Pass project_id to worker
            }
        )
        sends.append(send)

    return sends

def assign_estimation_workers(state: EnhancedOrchestratorState) -> List[Send]:
    """
    Ph√¢n c√¥ng estimation workers cho m·ªói breakdown task
    """
    breakdown_results = state.get('breakdown_results', [])
    project_id = state.get('project_id', None)
    logger.info(f"üìã ƒêang ph√¢n c√¥ng estimation workers cho {len(breakdown_results)} tasks")

    sends = []
    for task_breakdown in breakdown_results:
        send = Send(
            "estimation_worker",
            {
                "task_breakdown": task_breakdown,
                "project_id": project_id  # Pass project_id to worker
            }
        )
        sends.append(send)

    return sends

def assign_validation_workers(state: EnhancedOrchestratorState) -> List[Send]:
    """
    OPTION 3: Conditional validation - only validate high-risk tasks
    Ph√¢n c√¥ng validation workers CH·ªà cho tasks c·∫ßn validation
    """
    estimation_results = state.get('estimation_results', [])
    project_id = state.get('project_id', None)

    # Filter tasks that need validation
    tasks_needing_validation = [task for task in estimation_results if should_validate(task)]
    tasks_skipped = len(estimation_results) - len(tasks_needing_validation)

    logger.info(f"üìã Conditional validation: {len(tasks_needing_validation)} tasks need validation, {tasks_skipped} tasks skip validation")

    sends = []
    for estimation_task in tasks_needing_validation:
        send = Send(
            "validation_worker",
            {
                "estimation_task": estimation_task,
                "project_id": project_id  # Pass project_id to worker
            }
        )
        sends.append(send)

    return sends

# ========================
# Enhanced Synthesizer Node
# ========================

def enhanced_synthesizer_node(state: EnhancedOrchestratorState) -> Dict[str, Any]:
    """
    Enhanced Synthesizer v·ªõi advanced features
    NOW INCLUDES: Option 2 rule-based validation for tasks that skipped LLM validation
    """
    logger.info("üîÑ Enhanced Synthesizer ƒëang t·ªïng h·ª£p k·∫øt qu·∫£...")

    # Get results from both estimation and validation workers
    estimation_results = state.get('estimation_results', [])
    validated_results = state.get('validated_results', [])

    if not estimation_results and not validated_results:
        logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ results t·ª´ workers")
        return {
            'final_estimation_data': [],
            'total_effort': 0.0,
            'total_confidence': 0.0,
            'validation_summary': {},
            'workflow_status': 'no_results'
        }

    # Create validated task ID set
    validated_task_ids = {task.get('id') for task in validated_results}

    # Find tasks that skipped validation
    tasks_skipped_validation = [
        task for task in estimation_results
        if task.get('id') not in validated_task_ids
    ]

    # OPTION 2: Apply rule-based validation to tasks that skipped LLM validation
    if tasks_skipped_validation:
        logger.info(f"üìã Applying rule-based validation to {len(tasks_skipped_validation)} tasks that skipped LLM validation")
        rule_validated = apply_validation_rules(tasks_skipped_validation)
        validated_results.extend(rule_validated)

    if not validated_results:
        logger.warning("‚ö†Ô∏è No final validated results")
        return {
            'final_estimation_data': [],
            'total_effort': 0.0,
            'total_confidence': 0.0,
            'validation_summary': {},
            'workflow_status': 'no_results'
        }

    # T√≠nh to√°n summary statistics
    total_effort = sum(task.get('estimation_manday', 0) for task in validated_results)
    total_confidence = sum(task.get('confidence_level', 0) for task in validated_results) / len(validated_results)

    # T·∫°o validation summary
    validation_summary = {
        'total_tasks': len(validated_results),
        'total_effort_mandays': total_effort,
        'average_confidence': total_confidence,
        'categories_covered': list(set(task.get('category', 'Unknown') for task in validated_results)),
        'complexity_distribution': {},
        'risk_analysis': {
            'high_risk_tasks': [],
            'medium_risk_tasks': [],
            'low_risk_tasks': []
        },
        'adjustment_summary': {
            'tasks_adjusted': 0,
            'total_adjustment_mandays': 0,
            'adjustment_percentage': 0
        }
    }

    # Analyze complexity distribution
    complexity_counts = {}
    for task in validated_results:
        complexity = task.get('complexity', 'Medium')
        complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
    validation_summary['complexity_distribution'] = complexity_counts

    # Analyze risk factors
    for task in validated_results:
        risk_factors = task.get('risk_factors', [])
        confidence = task.get('confidence_level', 0.8)

        if len(risk_factors) > 2 or confidence < 0.6:
            validation_summary['risk_analysis']['high_risk_tasks'].append({
                'task': task.get('task_name', ''),
                'risks': risk_factors,
                'confidence': confidence
            })
        elif len(risk_factors) > 0 or confidence < 0.8:
            validation_summary['risk_analysis']['medium_risk_tasks'].append({
                'task': task.get('task_name', ''),
                'risks': risk_factors,
                'confidence': confidence
            })
        else:
            validation_summary['risk_analysis']['low_risk_tasks'].append({
                'task': task.get('task_name', ''),
                'confidence': confidence
            })

    # Calculate adjustment summary
    adjusted_tasks = 0
    total_adjustment = 0
    for task in validated_results:
        original_est = task.get('original_estimation', task.get('estimation_manday', 0))
        final_est = task.get('estimation_manday', 0)
        if abs(original_est - final_est) > 0.1:
            adjusted_tasks += 1
            total_adjustment += (final_est - original_est)

    validation_summary['adjustment_summary'] = {
        'tasks_adjusted': adjusted_tasks,
        'total_adjustment_mandays': total_adjustment,
        'adjustment_percentage': (total_adjustment / total_effort * 100) if total_effort > 0 else 0
    }

    # T·∫°o enhanced mermaid diagram
    mermaid_diagram = create_enhanced_mermaid_diagram(validated_results, validation_summary)

    logger.info(f"‚úÖ Enhanced Synthesizer ho√†n th√†nh:")
    logger.info(f"   - {len(validated_results)} tasks")
    logger.info(f"   - {total_effort:.1f} mandays total")
    logger.info(f"   - {total_confidence:.2f} average confidence")
    logger.info(f"   - {adjusted_tasks} tasks adjusted")

    return {
        'final_estimation_data': validated_results,
        'total_effort': total_effort,
        'total_confidence': total_confidence,
        'mermaid_diagram': mermaid_diagram,
        'validation_summary': validation_summary,
        'workflow_status': 'completed'
    }

# ========================
# Enhanced Mermaid Diagram Generator
# ========================

def create_enhanced_mermaid_diagram(validated_results: List[Dict[str, Any]], validation_summary: Dict[str, Any]) -> str:
    """
    T·∫°o enhanced mermaid diagram v·ªõi dependencies v√† risk indicators
    """
    # Extract summary info
    total_tasks = validation_summary.get('total_tasks', len(validated_results))
    total_effort = validation_summary.get('total_effort', 0)
    avg_confidence = validation_summary.get('average_confidence', 0)

    mermaid_code = f"""
graph TD
    A[Original Task] --> B[GraphRAG Analysis]
    B --> C[Task Breakdown]
    C --> D[Estimation]
    D --> E[Validation]
    E --> F["Final Results<br/>Tasks: {total_tasks}<br/>Total: {total_effort:.1f} days<br/>Confidence: {avg_confidence:.0%}"]

    %% Categories and Tasks
"""

    # Group by category
    categories = {}
    for task in validated_results:
        category = task.get('category', 'Unknown')
        if category not in categories:
            categories[category] = []
        categories[category].append(task)

    # Add categories and tasks
    for i, (category, tasks) in enumerate(categories.items()):
        cat_id = f"CAT{i}"
        total_effort = sum(task.get('estimation_manday', 0) for task in tasks)
        mermaid_code += f"    C --> {cat_id}[\"{category}<br/>{total_effort:.1f} days\"]\\n"

        # Add tasks for each category
        for j, task in enumerate(tasks):
            task_id = f"T{i}_{j}"
            task_name = task.get('task_name', 'Unknown Task')
            effort = task.get('estimation_manday', 0)
            confidence = task.get('confidence_level', 0)

            # Style based on risk/confidence
            if confidence < 0.6:
                style_class = "high-risk"
            elif confidence < 0.8:
                style_class = "medium-risk"
            else:
                style_class = "low-risk"

            mermaid_code += f"    {cat_id} --> {task_id}[\\\"{task_name}<br/>{effort:.1f}d ({confidence:.0%})\\\"]\\n"
            mermaid_code += f"    class {task_id} {style_class}\\n"

    # Add dependency arrows if they exist
    mermaid_code += "\n    %% Dependencies\\n"
    task_id_map = {}
    for i, (category, tasks) in enumerate(categories.items()):
        for j, task in enumerate(tasks):
            task_id_map[task.get('id', '')] = f"T{i}_{j}"

    for i, (category, tasks) in enumerate(categories.items()):
        for j, task in enumerate(tasks):
            dependencies = task.get('dependencies', [])
            current_task_id = f"T{i}_{j}"
            for dep_id in dependencies:
                if dep_id in task_id_map:
                    dep_task_id = task_id_map[dep_id]
                    mermaid_code += f"    {dep_task_id} -.-> {current_task_id}\\n"

    # Add styling
    mermaid_code += """

    %% Styling
    classDef high-risk fill:#ffebee,stroke:#f44336,stroke-width:2px
    classDef medium-risk fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef low-risk fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
"""

    return mermaid_code

# ========================
# Enhanced Excel Export Function
# ========================

def export_enhanced_excel(
    df: pd.DataFrame,
    validation_summary: Dict[str, Any],
    estimation_id: str = None,
    filename: str = None,
    format: str = "enhanced",
    no: str = "001",
    version: str = "1.0",
    issue_date: str = None,
    md_per_mm: int = 20,
    project_id: str = None
) -> Tuple[str, str]:
    """
    Enhanced Excel export v·ªõi detailed analysis.

    Args:
        df: DataFrame with estimation data
        validation_summary: Summary of validation results
        estimation_id: Unique estimation identifier (auto-generated if None)
        filename: Output filename (auto-generated if None)
        format: Export format - "enhanced" (default) or "sunasterisk"
        no: Document number (for Sun Asterisk format)
        version: Document version (for Sun Asterisk format)
        issue_date: Issue date (for Sun Asterisk format)
        md_per_mm: Man-days per man-month (for Sun Asterisk format)
        project_id: Project identifier for project-scoped storage

    Returns:
        Tuple[str, str]: (Path to exported Excel file, estimation_id)
    """
    from config import Config

    # Generate estimation_id if not provided
    if estimation_id is None:
        estimation_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    # Handle Sun Asterisk format
    if format == "sunasterisk":
        from utils.sunasterisk_excel_exporter import export_sunasterisk_excel

        # Convert DataFrame to Sun Asterisk format
        data = []
        for _, row in df.iterrows():
            task_dict = row.to_dict()

            # Convert to Sun Asterisk format
            sunasterisk_task = {
                'category': task_dict.get('category', ''),
                'parent_task': task_dict.get('parent_task', ''),
                'sub_task': task_dict.get('task_name', ''),  # Use task_name for business logic
                'sub_no': task_dict.get('sub_no', ''),
                'premise': task_dict.get('premise', ''),
                'remark': task_dict.get('remark', ''),
                'backend': {
                    'implement': task_dict.get('backend_implement', 0) or 0,
                    'fixbug': task_dict.get('backend_fixbug', 0) or 0,
                    'unittest': task_dict.get('backend_unittest', 0) or 0
                },
                'frontend': {
                    'implement': task_dict.get('frontend_implement', 0) or 0,
                    'fixbug': task_dict.get('frontend_fixbug', 0) or 0,
                    'unittest': task_dict.get('frontend_unittest', 0) or 0
                },
                'responsive': {
                    'implement': task_dict.get('responsive_implement', 0) or 0
                },
                'testing': {
                    'implement': task_dict.get('testing_implement', 0) or 0
                },
                'note': task_dict.get('note', '')
            }
            data.append(sunasterisk_task)

        # Export using Sun Asterisk exporter (with project_id support)
        sunasterisk_filepath = export_sunasterisk_excel(
            data=data,
            filename=filename,
            no=no,
            version=version,
            issue_date=issue_date,
            md_per_mm=md_per_mm,
            project_id=project_id
        )
        return sunasterisk_filepath, estimation_id

    # Original enhanced format
    # Auto-generate filename with new format if not provided
    if filename is None:
        filename = f"estimation_result_{estimation_id}.xlsx"

    # Determine result directory (project-scoped or default)
    if project_id:
        result_dir = Config.get_project_result_dir(project_id)
        logger.info(f"Using project-scoped result directory: {result_dir}")
    else:
        result_dir = Config.RESULT_EST_DIR
    
    # Ensure result_est directory exists
    os.makedirs(result_dir, exist_ok=True)

    # Save to result_est directory
    filepath = os.path.join(result_dir, filename)

    try:
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            # Main estimation table v·ªõi enhanced columns including role-specific estimations
            estimation_columns = [
                'id', 'category', 'parent_task', 'task_name', 'description',
                'estimation_manday',
                'confidence_level', 'complexity', 'priority',
                # Sun Asterisk detailed breakdown
                'sub_no', 'premise', 'remark', 'note',
                'backend_implement', 'backend_fixbug', 'backend_unittest',
                'frontend_implement', 'frontend_fixbug', 'frontend_unittest',
                'responsive_implement', 'testing_implement'
            ]

            # Filter columns that exist in DataFrame
            existing_columns = [col for col in estimation_columns if col in df.columns]
            df_filtered = df[existing_columns]
            df_filtered.to_excel(writer, sheet_name='Detailed Estimation', index=False)

            # Summary sheet
            summary_data = {
                'Metric': [
                    'Total Tasks',
                    'Total Effort (mandays)',
                    'Average Effort per Task',
                    'Average Confidence Level',
                    'Tasks Adjusted',
                    'Total Adjustment (mandays)',
                    'Adjustment Percentage',
                    'Generated Date'
                ],
                'Value': [
                    validation_summary.get('total_tasks', 0),
                    validation_summary.get('total_effort_mandays', 0),
                    validation_summary.get('total_effort_mandays', 0) / max(validation_summary.get('total_tasks', 1), 1),
                    validation_summary.get('average_confidence', 0),
                    validation_summary.get('adjustment_summary', {}).get('tasks_adjusted', 0),
                    validation_summary.get('adjustment_summary', {}).get('total_adjustment_mandays', 0),
                    f"{validation_summary.get('adjustment_summary', {}).get('adjustment_percentage', 0):.1f}%",
                    datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='Executive Summary', index=False)

            # Category breakdown
            if 'category' in df.columns and 'estimation_manday' in df.columns:
                category_summary = df.groupby('category').agg({
                    'estimation_manday': ['count', 'sum', 'mean'],
                    'confidence_level': 'mean'
                }).round(2)
                category_summary.columns = ['Task Count', 'Total Effort', 'Average Effort', 'Average Confidence']
                category_summary = category_summary.reset_index()
                category_summary.to_excel(writer, sheet_name='Category Analysis', index=False)

            # Role breakdown - Summary across all tasks
            role_summary_data = []
            
            # Backend total
            backend_total = (df['backend_implement'].sum() + 
                           df['backend_fixbug'].sum() + 
                           df['backend_unittest'].sum())
            
            # Frontend total (including responsive)
            frontend_total = (df['frontend_implement'].sum() + 
                            df['frontend_fixbug'].sum() + 
                            df['frontend_unittest'].sum() + 
                            df['responsive_implement'].sum())
            
            # Testing total
            testing_total = df['testing_implement'].sum()
            
            # Overall total
            overall_total = df['estimation_manday'].sum()
            
            if overall_total > 0:
                role_summary_data = [
                    {
                        'Role': 'Backend',
                        'Total Effort (mandays)': round(backend_total, 2),
                        'Percentage': f"{(backend_total / overall_total * 100):.1f}%",
                        'Tasks with Backend Work': df[df['backend_implement'] > 0].shape[0]
                    },
                    {
                        'Role': 'Frontend',
                        'Total Effort (mandays)': round(frontend_total, 2),
                        'Percentage': f"{(frontend_total / overall_total * 100):.1f}%",
                        'Tasks with Frontend Work': df[df['frontend_implement'] > 0].shape[0]
                    },
                    {
                        'Role': 'Testing/QA',
                        'Total Effort (mandays)': round(testing_total, 2),
                        'Percentage': f"{(testing_total / overall_total * 100):.1f}%",
                        'Tasks with Testing Work': df[df['testing_implement'] > 0].shape[0]
                    }
                ]
                
                role_summary_df = pd.DataFrame(role_summary_data)
                role_summary_df.to_excel(writer, sheet_name='Role Breakdown', index=False)

            # Risk analysis sheet
            risk_data = []
            for risk_level in ['high_risk_tasks', 'medium_risk_tasks', 'low_risk_tasks']:
                for task in validation_summary.get('risk_analysis', {}).get(risk_level, []):
                    risk_data.append({
                        'Risk Level': risk_level.replace('_tasks', '').replace('_', ' ').title(),
                        'Task Name': task.get('task', ''),
                        'Confidence': task.get('confidence', 0),
                        'Risk Factors': ', '.join(task.get('risks', []))
                    })

            if risk_data:
                risk_df = pd.DataFrame(risk_data)
                risk_df.to_excel(writer, sheet_name='Risk Analysis', index=False)

            # Complexity distribution
            complexity_data = []
            for complexity, count in validation_summary.get('complexity_distribution', {}).items():
                complexity_data.append({
                    'Complexity Level': complexity,
                    'Task Count': count,
                    'Percentage': f"{count / validation_summary.get('total_tasks', 1) * 100:.1f}%"
                })

            if complexity_data:
                complexity_df = pd.DataFrame(complexity_data)
                complexity_df.to_excel(writer, sheet_name='Complexity Distribution', index=False)

        logger.info(f"‚úÖ Enhanced Excel export completed: {filepath} (ID: {estimation_id})")
        return filepath, estimation_id

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi export Enhanced Excel: {e}")
        return "", estimation_id

# ========================
# Enhanced Workflow Builder
# ========================

class EnhancedEstimationWorkflow:
    """
    Enhanced Estimation Workflow v·ªõi specialized workers
    """

    def __init__(self, project_id: Optional[str] = None):
        """
        Initialize Enhanced Estimation Workflow
        
        Args:
            project_id: Optional project identifier for project-scoped operations.
                       When provided, all history and tracking will be project-specific.
        """
        self.project_id = project_id
        self.workflow = None
        self.memory = MemorySaver()
        self._build_workflow()

    def _build_workflow(self):
        """Build enhanced LangGraph workflow"""

        # T·∫°o StateGraph
        builder = StateGraph(EnhancedOrchestratorState)

        # Add nodes
        builder.add_node("enhanced_orchestrator", enhanced_orchestrator_node)
        builder.add_node("task_breakdown_worker", task_breakdown_worker)
        builder.add_node("estimation_worker", estimation_worker)
        builder.add_node("validation_worker", validation_worker)
        builder.add_node("enhanced_synthesizer", enhanced_synthesizer_node)

        # Add edges
        builder.add_edge(START, "enhanced_orchestrator")

        # Orchestrator -> Breakdown Workers
        builder.add_conditional_edges(
            "enhanced_orchestrator",
            assign_breakdown_workers,
            ["task_breakdown_worker"]
        )

        # Breakdown Workers -> Estimation Workers
        builder.add_conditional_edges(
            "task_breakdown_worker",
            assign_estimation_workers,
            ["estimation_worker"]
        )

        # Estimation Workers -> Validation Workers
        builder.add_conditional_edges(
            "estimation_worker",
            assign_validation_workers,
            ["validation_worker"]
        )

        # Validation Workers -> Synthesizer
        builder.add_edge("validation_worker", "enhanced_synthesizer")
        builder.add_edge("enhanced_synthesizer", END)

        # Compile workflow
        self.workflow = builder.compile(checkpointer=self.memory)

        logger.info("‚úÖ Enhanced Estimation Workflow ƒë√£ ƒë∆∞·ª£c build th√†nh c√¥ng!")

    def run_estimation(self, task_description: str, graphrag_insights=None, thread_id: str = None, project_id: str = None) -> Dict[str, Any]:
        """
        Ch·∫°y enhanced estimation workflow v·ªõi auto-generated estimation_id

        Args:
            task_description: Project description to estimate
            graphrag_insights: Optional GraphRAG insights
            thread_id: Optional custom thread ID (auto-generated if None)
            project_id: Optional project identifier for project-scoped operations.
                       If not provided, uses the instance's project_id.

        Returns:
            Dict with estimation results including estimation_id
        """
        # Use provided project_id or fall back to instance project_id
        effective_project_id = project_id or self.project_id
        
        # Generate estimation_id (timestamp-based)
        estimation_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Auto-generate thread_id if not provided
        if thread_id is None:
            thread_id = f"estimation_{estimation_id}"

        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu Enhanced Estimation Workflow (ID: {estimation_id})")
        if effective_project_id:
            logger.info(f"   Project ID: {effective_project_id}")
        logger.info(f"   Task: {task_description[:100]}...")

        initial_state = {
            "project_id": effective_project_id or "",  # Add project_id to state
            "estimation_id": estimation_id,  # NEW: Add estimation_id to state
            "original_task": task_description,
            "graphrag_insights": graphrag_insights or [],
            "main_categories": [],
            "breakdown_results": [],
            "estimation_results": [],
            "validated_results": [],
            "final_estimation_data": [],
            "total_effort": 0.0,
            "total_confidence": 0.0,
            "mermaid_diagram": "",
            "validation_summary": {},
            "workflow_status": "started"
        }

        try:
            # Run workflow
            config = {"configurable": {"thread_id": thread_id}}
            result = self.workflow.invoke(initial_state, config=config)

            # Ensure estimation_id and project_id are in result
            result['estimation_id'] = estimation_id
            result['project_id'] = effective_project_id or ""

            logger.info(f"üéâ Enhanced Workflow ho√†n th√†nh (ID: {estimation_id})")
            logger.info(f"   Status: {result.get('workflow_status', 'unknown')}")
            logger.info(f"   Total Effort: {result.get('total_effort', 0):.1f} mandays")

            return result

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ch·∫°y Enhanced Workflow (ID: {estimation_id}): {e}")
            return {
                "estimation_id": estimation_id,
                "project_id": effective_project_id or "",
                "workflow_status": "failed",
                "error": str(e)
            }

    def export_results(
        self,
        result: Dict[str, Any],
        filename: str = None,
        format: str = "sunasterisk",
        no: str = "001",
        version: str = "1.0",
        issue_date: str = None,
        md_per_mm: int = 20,
        project_id: str = None
    ) -> Tuple[str, str]:
        """
        Enhanced export k·∫øt qu·∫£ ra Excel v·ªõi SQLite tracking.

        Args:
            result: Workflow result dictionary
            filename: Output filename (auto-generated if None)
            format: Export format - "enhanced" (default) or "sunasterisk"
            no: Document number (for Sun Asterisk format)
            version: Document version (for Sun Asterisk format)
            issue_date: Issue date (for Sun Asterisk format)
            md_per_mm: Man-days per man-month (for Sun Asterisk format)
            project_id: Project identifier for project-scoped storage.
                       If not provided, uses result's project_id or instance's project_id.

        Returns:
            Tuple[str, str]: (Path to exported Excel file, estimation_id)
        """
        estimation_data = result.get('final_estimation_data', [])
        estimation_id = result.get('estimation_id', '')
        
        # Use provided project_id, or fall back to result's project_id, or instance's project_id
        effective_project_id = project_id or result.get('project_id') or self.project_id

        if not estimation_data:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ export")
            return "", estimation_id

        df = pd.DataFrame(estimation_data)
        validation_summary = result.get('validation_summary', {})

        # Export to Excel with project-scoped file path
        filepath, estimation_id = export_enhanced_excel(
            df=df,
            validation_summary=validation_summary,
            estimation_id=estimation_id,
            filename=filename,
            format=format,
            no=no,
            version=version,
            issue_date=issue_date,
            md_per_mm=md_per_mm,
            project_id=effective_project_id
        )

        # Save to SQLite tracker with project_id
        if filepath and estimation_id:
            try:
                from utils.estimation_result_tracker import get_result_tracker

                tracker = get_result_tracker()

                # Create estimation run entry with project_id
                tracker.create_estimation_run(
                    estimation_id=estimation_id,
                    file_path=filepath,
                    summary_data={
                        'total_effort': result.get('total_effort', 0.0),
                        'total_tasks': len(estimation_data),
                        'average_confidence': result.get('total_confidence', 0.0),
                        'workflow_status': result.get('workflow_status', 'completed'),
                        'project_description': result.get('original_task', '')
                    },
                    project_id=effective_project_id
                )

                # Save task details with project_id
                saved_count = tracker.save_estimation_tasks(
                    estimation_id=estimation_id,
                    tasks_data=estimation_data,
                    project_id=effective_project_id
                )

                if effective_project_id:
                    logger.info(f"‚úÖ Estimation {estimation_id} tracked in SQLite database for project {effective_project_id} ({saved_count} tasks)")
                else:
                    logger.info(f"‚úÖ Estimation {estimation_id} tracked in SQLite database ({saved_count} tasks)")

            except Exception as e:
                logger.error(f"‚ùå Failed to save to SQLite tracker: {e}")
                # Continue even if tracking fails - Excel export is still successful

        return filepath, estimation_id

    def get_mermaid_diagram(self, result: Dict[str, Any]) -> str:
        """
        L·∫•y enhanced mermaid diagram t·ª´ k·∫øt qu·∫£
        """
        return result.get('mermaid_diagram', '')

    def get_validation_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        L·∫•y validation summary t·ª´ k·∫øt qu·∫£
        """
        return result.get('validation_summary', {})

    def visualize_workflow(self) -> str:
        """
        T·∫°o visualization c·ªßa enhanced workflow graph
        """
        try:
            # Get workflow graph
            graph = self.workflow.get_graph()
            mermaid_png = graph.draw_mermaid_png()

            # Save to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"enhanced_workflow_diagram_{timestamp}.png"

            with open(filename, 'wb') as f:
                f.write(mermaid_png)

            logger.info(f"‚úÖ ƒê√£ t·∫°o enhanced workflow diagram: {filename}")
            return filename

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o enhanced workflow diagram: {e}")
            return ""

# ========================
# Usage Example
# ========================

if __name__ == "__main__":
    # Kh·ªüi t·∫°o enhanced workflow
    enhanced_workflow = EnhancedEstimationWorkflow()

    # Example task
    sample_task = """
    Ph√°t tri·ªÉn m·ªôt ·ª©ng d·ª•ng web e-commerce ho√†n ch·ªânh v·ªõi c√°c t√≠nh nƒÉng:
    - Qu·∫£n l√Ω s·∫£n ph·∫©m (CRUD) v·ªõi image upload
    - Gi·ªè h√†ng v√† thanh to√°n v·ªõi multiple payment methods
    - Qu·∫£n l√Ω ng∆∞·ªùi d√πng v√† authentication v·ªõi social login
    - Admin dashboard v·ªõi analytics
    - Responsive design cho mobile v√† desktop
    - Payment gateway integration (Stripe, PayPal)
    - Email notifications v√† SMS alerts
    - Advanced search v√† filtering v·ªõi Elasticsearch
    - Product recommendations v·ªõi ML
    - Multi-language support
    - Real-time chat support
    """

    # Ch·∫°y estimation (without GraphRAG for this example)
    result = enhanced_workflow.run_estimation(sample_task, graphrag_insights=None)

    # Xu·∫•t k·∫øt qu·∫£
    if result.get('workflow_status') == 'completed':
        excel_file = enhanced_workflow.export_results(result, format="sunasterisk")
        mermaid_diagram = enhanced_workflow.get_mermaid_diagram(result)
        validation_summary = enhanced_workflow.get_validation_summary(result)

        logger.info(f"\nüìä Enhanced Estimation Results:")
        logger.info(f"- Total effort: {result.get('total_effort', 0):.1f} mandays")
        logger.info(f"- Average confidence: {result.get('total_confidence', 0):.2f}")
        logger.info(f"- Tasks processed: {len(result.get('final_estimation_data', []))}")
        logger.info(f"- Excel file: {excel_file}")
        logger.info(f"- Tasks adjusted: {validation_summary.get('adjustment_summary', {}).get('tasks_adjusted', 0)}")

        logger.info(f"\nüé® Enhanced Mermaid Diagram:\n{mermaid_diagram}")

    # T·∫°o workflow visualization
    workflow_diagram = enhanced_workflow.visualize_workflow()